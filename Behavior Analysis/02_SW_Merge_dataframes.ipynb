{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gal_35_father_retreive_trial_1', 'Gal_38_mother_retreive_trial_1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#directory where behavior videos are located\n",
    "video_directory = '01_Raw_videos'\n",
    "Adult_model_directory = '02_Adult_model'\n",
    "Infant_model_adjusted_directory = '04_Infants_model_adjusted'\n",
    "Nest_model_directory = '05_Nest_model'\n",
    "output_directory = '06_Unified_dataframe'\n",
    "file_location = os.path.join(video_directory, '*.mp4')\n",
    "\n",
    "# create list of filenames for every video to be processed\n",
    "# this list is the input for the video processing function\n",
    "filenames = glob.glob(file_location)\n",
    "videoname = []\n",
    "for f in filenames:\n",
    "    \n",
    "    # establish name for output file from the input filename\n",
    "    x = os.path.split(f)\n",
    "    x = x[1].split('.mp4')\n",
    "    x = x[0]\n",
    "    videoname.append(x)\n",
    "videoname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and combine data from DeepLabCut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wrigh\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2490: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->MultiIndex([('Nest', '0', 'nest_0_0'),\n",
      "            ('Nest', '0', 'nest_0_1'),\n",
      "            ('Nest', '0', 'nest_0_2'),\n",
      "            ('Nest', '1', 'nest_1_0'),\n",
      "            ('Nest', '2', 'nest_2_0')],\n",
      "           names=['individuals', 'bodyparts', 'coords'])]\n",
      "\n",
      "  pytables.to_hdf(\n",
      "C:\\Users\\wrigh\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2490: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->MultiIndex([('Nest', '0', 'nest_0_0'),\n",
      "            ('Nest', '0', 'nest_0_1'),\n",
      "            ('Nest', '0', 'nest_0_2'),\n",
      "            ('Nest', '1', 'nest_1_0')],\n",
      "           names=['individuals', 'bodyparts', 'coords'])]\n",
      "\n",
      "  pytables.to_hdf(\n"
     ]
    }
   ],
   "source": [
    "for x in videoname:\n",
    "    #Import the H5 file for Infants\n",
    "    Infant_file = os.path.join(Infant_model_adjusted_directory, x)\n",
    "    Infant_var = glob.glob(Infant_file + '*')\n",
    "    Infants = pd.read_hdf(Infant_var[0])\n",
    "    #Infants = Infants.droplevel(level=0, axis=1)\n",
    "    Infants = Infants.apply(pd.to_numeric)\n",
    "    Infant_bodyparts = Infants.columns.get_level_values(level=1)\n",
    "    Infant_bodyparts = set(Infant_bodyparts.tolist())\n",
    "    \n",
    "    #Import the H5 file for Adults\n",
    "    Adult_file = os.path.join(Adult_model_directory, x)\n",
    "    Adult_var = glob.glob(Adult_file + '*')\n",
    "    Adults = pd.read_hdf(Adult_var[0])\n",
    "    Adults = Adults.droplevel(level=0, axis=1)\n",
    "    Adults = Adults.apply(pd.to_numeric)\n",
    "    # The Adult DLC dataframe was created using single animal DLC so there is no multiindex level cooresponding to 'individuals'\n",
    "    # Must make the indexes match before we can merge.\n",
    "    Adults['individuals'] = 'Adult'\n",
    "    Adults = Adults.set_index('individuals', append=True).unstack('individuals')\n",
    "    Adults = Adults.reorder_levels([2,0,1], axis=1)\n",
    "    Adult_bodyparts = Adults.columns.get_level_values(level=1)\n",
    "    Adult_bodyparts = set(Adult_bodyparts.tolist())\n",
    "    \n",
    "    #Import the H5 file for Nests\n",
    "    Nest_file = os.path.join(Nest_model_directory, x)\n",
    "    Nest_var = glob.glob(Nest_file + '*')\n",
    "    Nest = pd.read_hdf(Nest_var[0])\n",
    "    # Determine the maxium amount of bounding boxes present in a single frame\n",
    "    # This will \n",
    "    nest_columns = Nest.columns\n",
    "\n",
    "    nest_list = set()\n",
    "    for c in nest_columns:\n",
    "        nest_number = str([c])\n",
    "        nest_number = nest_number[7]\n",
    "        nest_list.add(nest_number)\n",
    "    \n",
    "    #Tidy the nominclature\n",
    "    for b in nest_list:\n",
    "        nest_number = b\n",
    "        if 'nest_{}_0'.format(b) in nest_columns:\n",
    "            Nest['nest_{}_0'.format(b)] = Nest['nest_{}_0'.format(b)].combine_first(Nest['nest_{}'.format(b)])\n",
    "        else:\n",
    "            Nest['nest_{}_0'.format(b)] = Nest['nest_{}'.format(b)]\n",
    "        Nest.drop(['nest_{}'.format(b)], axis=1, inplace=True)\n",
    "\n",
    "    #NEST['Frame'] = np.arange(len(NEST))    \n",
    "    Nest = Nest.reindex(sorted(Nest.columns), axis=1)\n",
    "\n",
    "    # Create a list to be used when making the MultiIndex#\n",
    "    # Cooresponds to the bounding box that the polygon belongs to\n",
    "    bounding_box_number = []\n",
    "    for a in Nest.columns:\n",
    "        test = a[5]\n",
    "        bounding_box_number.append(test)\n",
    "\n",
    "    index = []\n",
    "    for z in Nest.columns:\n",
    "        var = ('Nest', z[5], z,)\n",
    "        index.append(var)\n",
    "    \n",
    "    multi = pd.MultiIndex.from_tuples(index, names=['individuals', 'bodyparts', 'coords'])\n",
    "\n",
    "    Nest.columns = multi\n",
    "    \n",
    "    # Combine all three dataframes\n",
    "    Full = pd.concat([Adults, Infants, Nest], axis=1)\n",
    "\n",
    "    # Save output to HDf5\n",
    "    h5File = (x + \".\" +'h5')\n",
    "    h5File = os.path.join(output_directory, h5File)\n",
    "    Full.to_hdf(h5File, key=\"NEST\", mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>individuals</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Infant1</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Infant2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bodyparts</th>\n",
       "      <th colspan=\"3\" halign=\"left\">head</th>\n",
       "      <th colspan=\"3\" halign=\"left\">middle_head</th>\n",
       "      <th colspan=\"3\" halign=\"left\">middle_tail</th>\n",
       "      <th>tail</th>\n",
       "      <th>...</th>\n",
       "      <th>head</th>\n",
       "      <th colspan=\"3\" halign=\"left\">middle_head</th>\n",
       "      <th colspan=\"3\" halign=\"left\">middle_tail</th>\n",
       "      <th colspan=\"3\" halign=\"left\">tail</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coords</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>...</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13951</th>\n",
       "      <td>176.0</td>\n",
       "      <td>169.25</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>166.875</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>164.0</td>\n",
       "      <td>161.625</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>164.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>145.25</td>\n",
       "      <td>105.0625</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>111.125</td>\n",
       "      <td>159.0</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>112.875</td>\n",
       "      <td>167.875</td>\n",
       "      <td>0.010002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13952</th>\n",
       "      <td>176.0</td>\n",
       "      <td>169.25</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>166.875</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>164.0</td>\n",
       "      <td>161.625</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>164.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>145.25</td>\n",
       "      <td>105.0625</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>111.125</td>\n",
       "      <td>159.0</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>112.875</td>\n",
       "      <td>167.875</td>\n",
       "      <td>0.010002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13953</th>\n",
       "      <td>176.0</td>\n",
       "      <td>169.25</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>166.875</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>164.0</td>\n",
       "      <td>161.625</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>164.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>145.25</td>\n",
       "      <td>105.0625</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>111.125</td>\n",
       "      <td>159.0</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>112.875</td>\n",
       "      <td>167.875</td>\n",
       "      <td>0.010002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13954</th>\n",
       "      <td>176.0</td>\n",
       "      <td>169.25</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>166.875</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>164.0</td>\n",
       "      <td>161.625</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>164.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>145.25</td>\n",
       "      <td>105.0625</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>111.125</td>\n",
       "      <td>159.0</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>112.875</td>\n",
       "      <td>167.875</td>\n",
       "      <td>0.010002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13955</th>\n",
       "      <td>176.0</td>\n",
       "      <td>169.25</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>166.875</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>164.0</td>\n",
       "      <td>161.625</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>164.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>145.25</td>\n",
       "      <td>105.0625</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>111.125</td>\n",
       "      <td>159.0</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>112.875</td>\n",
       "      <td>167.875</td>\n",
       "      <td>0.010002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13956 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "individuals Infant1                                                   \\\n",
       "bodyparts      head                    middle_head                     \n",
       "coords            x       y likelihood           x      y likelihood   \n",
       "0               NaN     NaN        NaN         NaN    NaN        NaN   \n",
       "1               NaN     NaN        NaN         NaN    NaN        NaN   \n",
       "2               NaN     NaN        NaN         NaN    NaN        NaN   \n",
       "3               NaN     NaN        NaN         NaN    NaN        NaN   \n",
       "4               NaN     NaN        NaN         NaN    NaN        NaN   \n",
       "...             ...     ...        ...         ...    ...        ...   \n",
       "13951         176.0  169.25   0.010002     166.875  164.0   0.010002   \n",
       "13952         176.0  169.25   0.010002     166.875  164.0   0.010002   \n",
       "13953         176.0  169.25   0.010002     166.875  164.0   0.010002   \n",
       "13954         176.0  169.25   0.010002     166.875  164.0   0.010002   \n",
       "13955         176.0  169.25   0.010002     166.875  164.0   0.010002   \n",
       "\n",
       "individuals                                         ...    Infant2  \\\n",
       "bodyparts   middle_tail                       tail  ...       head   \n",
       "coords                x        y likelihood      x  ... likelihood   \n",
       "0                   NaN      NaN        NaN    NaN  ...        NaN   \n",
       "1                   NaN      NaN        NaN    NaN  ...        NaN   \n",
       "2                   NaN      NaN        NaN    NaN  ...        NaN   \n",
       "3                   NaN      NaN        NaN    NaN  ...        NaN   \n",
       "4                   NaN      NaN        NaN    NaN  ...        NaN   \n",
       "...                 ...      ...        ...    ...  ...        ...   \n",
       "13951             164.0  161.625   0.010002  164.5  ...   0.010002   \n",
       "13952             164.0  161.625   0.010002  164.5  ...   0.010002   \n",
       "13953             164.0  161.625   0.010002  164.5  ...   0.010002   \n",
       "13954             164.0  161.625   0.010002  164.5  ...   0.010002   \n",
       "13955             164.0  161.625   0.010002  164.5  ...   0.010002   \n",
       "\n",
       "individuals                                                                 \\\n",
       "bodyparts   middle_head                      middle_tail                     \n",
       "coords                x         y likelihood           x      y likelihood   \n",
       "0                   NaN       NaN        NaN         NaN    NaN        NaN   \n",
       "1                   NaN       NaN        NaN         NaN    NaN        NaN   \n",
       "2                   NaN       NaN        NaN         NaN    NaN        NaN   \n",
       "3                   NaN       NaN        NaN         NaN    NaN        NaN   \n",
       "4                   NaN       NaN        NaN         NaN    NaN        NaN   \n",
       "...                 ...       ...        ...         ...    ...        ...   \n",
       "13951            145.25  105.0625   0.010002     111.125  159.0   0.010002   \n",
       "13952            145.25  105.0625   0.010002     111.125  159.0   0.010002   \n",
       "13953            145.25  105.0625   0.010002     111.125  159.0   0.010002   \n",
       "13954            145.25  105.0625   0.010002     111.125  159.0   0.010002   \n",
       "13955            145.25  105.0625   0.010002     111.125  159.0   0.010002   \n",
       "\n",
       "individuals                               \n",
       "bodyparts       tail                      \n",
       "coords             x        y likelihood  \n",
       "0                NaN      NaN        NaN  \n",
       "1                NaN      NaN        NaN  \n",
       "2                NaN      NaN        NaN  \n",
       "3                NaN      NaN        NaN  \n",
       "4                NaN      NaN        NaN  \n",
       "...              ...      ...        ...  \n",
       "13951        112.875  167.875   0.010002  \n",
       "13952        112.875  167.875   0.010002  \n",
       "13953        112.875  167.875   0.010002  \n",
       "13954        112.875  167.875   0.010002  \n",
       "13955        112.875  167.875   0.010002  \n",
       "\n",
       "[13956 rows x 24 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
